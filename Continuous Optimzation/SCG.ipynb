{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from keras import backend as K\n",
    "from keras.models import Sequential\n",
    "from keras import losses\n",
    "from keras.layers import Dense, Activation\n",
    "from sklearn import datasets\n",
    "from sklearn.preprocessing import OneHotEncoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 讀取資料\n",
    "* 將資料分為輸入特徵及類別\n",
    "* 將多分類問題轉為二元分類"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris = datasets.load_iris()\n",
    "X = iris.data\n",
    "Y = iris.target\n",
    "Y[Y == 2] = 1\n",
    "Y = Y.reshape(-1, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 共軛梯度涵式\n",
    "* 建立神經網路\n",
    "* 取得網路權重\n",
    "* 設定網路權重\n",
    "* 取得當前梯度\n",
    "* 進行梯度壓縮\n",
    "* 還原壓縮梯度\n",
    "* 取得解空間的長度\n",
    "* 更新網路權重"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 建立神經網路\n",
    "\n",
    "def create_model():\n",
    "    model = Sequential()\n",
    "    model.add(Dense(units = 5, activation = 'relu', input_dim = X.shape[1])) \n",
    "    model.add(Dense(units = 1, activation = 'sigmoid'))\n",
    "    model.compile(loss = 'binary_crossentropy', optimizer = 'adam')\n",
    "    \n",
    "    return(model)\n",
    "\n",
    "# 取得網路權重\n",
    "\n",
    "def get_weight(model):\n",
    "    return(model.get_weights())\n",
    "\n",
    "# 設定網路權重\n",
    "\n",
    "def set_weight(model, weights):\n",
    "    return(model.set_weights(weights))\n",
    "\n",
    "# 取得當前梯度\n",
    "\n",
    "def get_weight_grad(model, inputs, outputs):\n",
    "    grads = model.optimizer.get_gradients(model.total_loss, model.trainable_weights)\n",
    "    symb_inputs = (model._feed_inputs + model._feed_targets + model._feed_sample_weights)\n",
    "    f = K.function(symb_inputs, grads)\n",
    "    x, y, sample_weight = model._standardize_user_data(inputs, outputs)\n",
    "    output_grad = f(x + y + sample_weight)\n",
    "    return output_grad\n",
    "\n",
    "# 進行梯度壓縮\n",
    "\n",
    "def flatten_grad(grad):\n",
    "    shape_list = list()\n",
    "    range_list = list()\n",
    "    value_list = list()\n",
    "    \n",
    "    for i in range(len(grad)):\n",
    "        shape_list.append(np.array(grad[i]).shape)\n",
    "        if(i == 0):\n",
    "            range_list.append(np.array(grad[i]).reshape(-1).shape[0])\n",
    "        else:\n",
    "            range_list.append(np.array(grad[i]).reshape(-1).shape[0] + range_list[i - 1])\n",
    "            \n",
    "    for i in range(len(grad)):\n",
    "        value_list = value_list + np.array(grad[i]).reshape(-1).tolist()\n",
    "        \n",
    "    return shape_list, range_list, value_list\n",
    "\n",
    "# 還原壓縮梯度\n",
    "\n",
    "def construct_grad(shape_list, range_list, value_list):\n",
    "    grad = list()\n",
    "    for i in range(len(shape_list)):\n",
    "        beg = 0\n",
    "        end = range_list[i]\n",
    "        if(i > 0):\n",
    "            beg = range_list[i - 1]\n",
    "\n",
    "        grad.append(np.array(value_list[beg:end]).reshape(shape_list[i]))\n",
    "        \n",
    "    return(grad)\n",
    "\n",
    "# 取得解空間的長度\n",
    "\n",
    "def get_weight_length(value_list):\n",
    "    length = 0\n",
    "    \n",
    "    for i in range(len(value_list)):\n",
    "        length = length + np.power(value_list[i], 2)\n",
    "    \n",
    "    return(np.sqrt(length))\n",
    "\n",
    "# 更新網路權重\n",
    "\n",
    "def step(weight, grad):\n",
    "    update_weight = weight\n",
    "    for i in range(len(weight)):\n",
    "        update_weight[i] = weight[i] + grad[i]\n",
    "    \n",
    "    return(update_weight)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 共軛伸縮梯度下降步驟\n",
    "* 步驟1: 初始化\n",
    "* 步驟2: 計算二階梯度資訊\n",
    "* 步驟3: 進行 delta k 伸縮\n",
    "* 步驟4: 如果 delta_k <= 0, 讓 hessian 轉為正定\n",
    "* 步驟5: 計算移動步長 alpha\n",
    "* 步驟6: 計算比較參數 comp\n",
    "* 步驟7: 若 comp >= 0, 表示損失函數可以降低\n",
    "* 步驟8: 若 comp < 0.25, 增加伸縮尺度 scale\n",
    "* 步驟9: 若梯度等於 0, 表示最佳解尋得, 停止演算法"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scaled_conjugate_gradient(model, inputs, outputs, maxiter):\n",
    "    \n",
    "    # 步驟1: 初始化\n",
    "\n",
    "    sigma = 1e-4\n",
    "    lambda_ = 1e-6\n",
    "    lambda_hat = 0\n",
    "    p = [(grad * -1) for grad in get_weight_grad(model, X, Y)]\n",
    "    p_prior = p\n",
    "    shape_list, range_list, value_list = flatten_grad(p)\n",
    "    p_flatten = np.array(value_list)\n",
    "    p_flatten_prior = p_flatten\n",
    "    r = p_flatten\n",
    "    r_prior = r\n",
    "    N = len(value_list)\n",
    "    success = True\n",
    "\n",
    "    for k in range(1, maxiter + 1):\n",
    "\n",
    "        # 步驟2: 計算二階梯度資訊\n",
    "\n",
    "        model_appr = create_model()\n",
    "        sigma_k = sigma / get_weight_length(p_flatten)\n",
    "        set_weight(model_appr, step(get_weight(model), [grad * sigma_k for grad in p]))\n",
    "\n",
    "        grad_appr = get_weight_grad(model_appr, X, Y)\n",
    "        grad_curr = get_weight_grad(model, X, Y)\n",
    "        _shape_list, _range_list, grad_appr = np.array(flatten_grad(grad_appr))\n",
    "        _shape_list, _range_list, grad_curr = np.array(flatten_grad(grad_curr))\n",
    "        grad_appr = np.array(grad_appr)\n",
    "        grad_curr = np.array(grad_curr)\n",
    "\n",
    "        s = (grad_appr - grad_curr) / sigma_k\n",
    "        delta = np.dot(p_flatten, s)\n",
    "\n",
    "        # 步驟3: 進行 delta k 伸縮\n",
    "\n",
    "        delta = delta + (lambda_ - lambda_hat) * np.dot(p_flatten, p_flatten)\n",
    "\n",
    "        # 步驟4: 如果 delta_k <= 0, 讓 hessian 轉為正定\n",
    "\n",
    "        if(delta <= 0):\n",
    "            lambda_hat = 2 * (lambda_ - (delta / np.dot(p_flatten, p_flatten)))\n",
    "            delta = -delta + (lambda_ * np.dot(p_flatten, p_flatten))\n",
    "            lambda_ = lambda_hat\n",
    "\n",
    "        # 步驟5: 計算移動步長 alpha\n",
    "\n",
    "        mu = np.dot(p_flatten, r)\n",
    "        alpha = mu / delta\n",
    "\n",
    "        # 步驟6: 計算比較參數 comp\n",
    "\n",
    "        set_weight(model_appr, step(get_weight(model), [grad * alpha for grad in p]))\n",
    "        loss_curr = model.evaluate(X, Y)\n",
    "        loss_appr = model_appr.evaluate(X, Y)\n",
    "        comp = 2 * (loss_curr - loss_appr) / np.power(mu, 2)\n",
    "\n",
    "        # 步驟7: 若 comp >= 0, 表示損失函數可以降低\n",
    "\n",
    "        if(comp >= 0):\n",
    "            set_weight(model, step(get_weight(model), [grad * alpha for grad in p]))\n",
    "            r_prior = r\n",
    "            r = [(grad * -1) for grad in get_weight_grad(model, X, Y)]\n",
    "            lambda_hat = 0\n",
    "            success = True\n",
    "\n",
    "            if(k % N == 0):\n",
    "                p_prior = p\n",
    "                p = r\n",
    "                shape_list, range_list, r = flatten_grad(r)\n",
    "                p_flatten = r\n",
    "            else:\n",
    "                shape_list, range_list, r = flatten_grad(r)\n",
    "                beta = ((np.dot(r, r) - np.dot(r, r_prior)) / mu)\n",
    "                p_prior = p\n",
    "                p = step(construct_grad(shape_list, range_list, r), [(weight * beta) for weight in p])\n",
    "                p_flatten_prior = p_flatten\n",
    "                shape_list, range_list, p_flatten = flatten_grad(p)\n",
    "\n",
    "            if(comp >= 0.75):\n",
    "                lambda_ = lambda_ / 4\n",
    "        else:\n",
    "            lambda_hat = lambda_\n",
    "            success = False\n",
    "\n",
    "        # 步驟8: 若 comp < 0.25, 增加伸縮尺度 scale\n",
    "\n",
    "        if(comp < 0.25):\n",
    "            lambda_ = lambda_ + (delta * (1 - comp) / np.dot(p_flatten_prior, p_flatten_prior))\n",
    "\n",
    "        # 步驟9: 若梯度等於 0, 表示最佳解尋得, 停止演算法\n",
    "\n",
    "        print(comp)\n",
    "        if(np.array_equal(r, np.zeros(len(r)))):\n",
    "            print('converge to local optima')\n",
    "            break;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 建立神經網路模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = create_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 使用共軛梯度下降優化網路權重"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "150/150 [==============================] - 1s 4ms/step\n",
      "150/150 [==============================] - 1s 4ms/step\n",
      "0.621233856561797\n",
      "150/150 [==============================] - 0s 104us/step\n",
      "150/150 [==============================] - 1s 4ms/step\n",
      "-59.404584952096556\n",
      "150/150 [==============================] - 0s 0us/step\n",
      "150/150 [==============================] - 1s 4ms/step\n",
      "4.271329610137764\n",
      "150/150 [==============================] - 0s 0us/step\n",
      "150/150 [==============================] - 1s 4ms/step\n",
      "5.657324350621068\n",
      "150/150 [==============================] - 0s 104us/step\n",
      "150/150 [==============================] - 1s 5ms/step\n",
      "101.04409448855539\n",
      "150/150 [==============================] - 0s 0us/step\n",
      "150/150 [==============================] - 1s 5ms/step\n",
      "0.2993012323401148\n",
      "150/150 [==============================] - 0s 0us/step\n",
      "150/150 [==============================] - 1s 5ms/step\n",
      "26.868836975607074\n",
      "150/150 [==============================] - 0s 0us/step\n",
      "150/150 [==============================] - 1s 5ms/step\n",
      "271.9907103646248\n",
      "150/150 [==============================] - 0s 0us/step\n",
      "150/150 [==============================] - 1s 5ms/step\n",
      "535.0497201084588\n",
      "150/150 [==============================] - 0s 104us/step\n",
      "150/150 [==============================] - 1s 5ms/step\n",
      "766.88575004283\n"
     ]
    }
   ],
   "source": [
    "scaled_conjugate_gradient(model, X, Y, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 使用準確率評估模型的表現"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: [100.]%\n"
     ]
    }
   ],
   "source": [
    "print('accuracy: {0}%'.format((sum(np.round(model.predict(X)) == Y) / len(Y)) * 100))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
